2019-01-03 15:37:59,012 [INFO ]  Log file is /apps/yinhong/gitsource/seglink/exp/sgd/finetune_ic15_20190103_153759.log
2019-01-03 15:37:59,012 [INFO ]  Parameters:
'--resize_longer_side': 512
'--neg_scale_diff_threshold': 2.0
'--detailed_summary_period': 200
'--image_height': 512
'--finetune_model': ../premodel/ILVSR_VGG_16_FC_REDUCED/VGG_ILSVRC_16_layers_ssd.ckpt
'--image_width': 512
'--log_dir': /apps/yinhong/gitsource/seglink/exp/sgd
'--profiling': 0
'--log_prefix': finetune_ic15_
'--hard_neg_ratio': 3.0
'--resume': vgg16
'--no_random_crop': 0
'--image_channel_order': BGR
'--max_steps': 10000
'--train_datasets': ../data/icdar_2015_incidental_train.tf
'--vgg16_model': ../premodel/ILVSR_VGG_16_FC_REDUCED/VGG_ILSVRC_16_layers_ssd.ckpt
'--weight_init_method': xavier
'--profiling_step': 21
'--pos_scale_diff_threshold': 1.7
'--weight_decay': 0.0005
'--lr_decays': 1.0,0.1,0.01
'--lr_policy': staircase
'--n_gpu': 2
'--resize_step': 128
'--optimizer': sgd
'--base_lr': 0.0001
'--sampling_overlap_mode': coverage
'--train_batch_size': 32
'--lr_breakpoints': 2000,4000,6000
'--checkpoint_period': 1000
'--momentum': 0.9
'--profiling_report': timeline.json
'--brief_summary_period': 20
'--test_resize_method': fixed
'--max_num_gt': 300
2019-01-03 15:37:59,017 [INFO ]  Git commit is b'd8edb9e'
2019-01-03 15:37:59,255 [INFO ]  Uncommitted chanages:
b'diff --git a/README.md b/README.md\nindex ea7216e..c2cb147 100644\n--- a/README.md\n+++ b/README.md\n@@ -36,3 +36,9 @@ See ``tool/create_datasets.py\'\'\n ## Evaluation\n \n See ``evaluate.py\'\'\n+\n+##\xe6\x95\xb0\xe6\x8d\xae\xe4\xb8\x8b\xe8\xbd\xbd\xe5\x92\x8c\xe6\xa8\xa1\xe5\x9e\x8bpretrain\n+\xe7\xbd\x91\xe5\x9d\x80:https://github.com/intel/caffe/tree/master/models/intel_optimized_models/ssd/VGGNet\n+./build/tools/caffe train -solver models/intel_optimized_models/ssd/VGGNet/VOC0712/SSD_300x300/solver.prototxt \\\n+-weights models/intel_optimized_models/ssd/VGGNet/VGG_ILSVRC_16_layers_fc_reduced.caffemodel\n+\ndiff --git a/exp/sgd/finetune_ic15.json b/exp/sgd/finetune_ic15.json\nindex 3f89924..3a6d4a2 100644\n--- a/exp/sgd/finetune_ic15.json\n+++ b/exp/sgd/finetune_ic15.json\n@@ -1,8 +1,9 @@\n {\n   "cuda_devices": "2,3",\n   "log_prefix": "finetune_ic15_",\n-  "resume": "finetune",\n-  "finetune_model": "../exp/sgd/checkpoint",\n+  "resume": "vgg16",\n+  "finetune_model": "../premodel/ILVSR_VGG_16_FC_REDUCED/VGG_ILSVRC_16_layers_ssd.ckpt",\n+  "vgg16_model": "../premodel/ILVSR_VGG_16_FC_REDUCED/VGG_ILSVRC_16_layers_ssd.ckpt",\n   "train_datasets": "../data/icdar_2015_incidental_train.tf",\n   "image_height": 512,\n   "image_width": 512,\ndiff --git a/exp/sgd/pretrain.json b/exp/sgd/pretrain.json\nindex a588d04..661937b 100644\n--- a/exp/sgd/pretrain.json\n+++ b/exp/sgd/pretrain.json\n@@ -2,7 +2,7 @@\n   "cuda_devices": "0,1",\n   "log_prefix": "pretrain_",\n   "pretrained_model": "../data/VGG_ILSVRC_16_layers_ssd.ckpt",\n-  "train_datasets": "../data/synthtext_full.tf",\n+  "train_datasets": "../data/icdar_2015_incidental_train.tf",\n   "image_height": 384,\n   "image_width": 384,\n   "resume_training": 0,\ndiff --git a/manage.py b/manage.py\nindex 5f03e6a..50bcc60 100755\n--- a/manage.py\n+++ b/manage.py\n@@ -8,7 +8,7 @@ import glob\n SRC_DIR = abspath(\'./seglink\')\n SHARED_LIBRARY_NAME = \'libseglink.so\'\n \n-\n+# \xe7\xbc\x96\xe8\xaf\x91\n def build_op():\n   build_dir = join(SRC_DIR, \'cpp/build\')\n   if not exists(build_dir):\n@@ -16,18 +16,19 @@ def build_op():\n   os.chdir(build_dir)\n   if not exists(\'Makefile\'):\n     os.system(\'cmake -DCMAKE_BUILD_TYPE=Release ..\')\n+  # \xe5\x8d\x81\xe5\x85\xad\xe4\xb8\xaacpu\n   os.system(\'make -j16\')\n   os.system(\'cp %s %s\' % (SHARED_LIBRARY_NAME, join(SRC_DIR, SHARED_LIBRARY_NAME)))\n   print(\'Building complete\')\n \n-\n+# \xe9\x80\x92\xe5\xbd\x92\xe5\x88\xa0\xe9\x99\xa4\xe5\xbb\xba\xe7\xab\x8b\xe7\x9a\x84\xe7\xbc\x96\xe8\xaf\x91\xe6\x96\x87\xe4\xbb\xb6\n def clean_op():\n   build_dir = join(SRC_DIR, \'cpp/build\')\n   print(\'Deleting recursively: %s\' % build_dir)\n   os.system(\'rm -rI %s\' % build_dir)\n   os.system(\'rm %s\' % join(SRC_DIR, SHARED_LIBRARY_NAME))\n \n-\n+# \xe6\xb8\x85\xe7\x90\x86\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84json\xe6\x96\x87\xe4\xbb\xb6\n def clear():\n   if len(sys.argv) != 3:\n     print(\'Usage ./manage.py clear <exp_dir>\')\n@@ -47,7 +48,7 @@ def clear():\n     os.remove(fpath)\n   print(\'Deleted {} files\'.format(len(files_to_delete)))\n \n-\n+# \xe4\xbd\xbf\xe7\x94\xa8.json\xe9\x85\x8d\xe7\xbd\xae\xe6\x96\x87\xe4\xbb\xb6\xef\xbc\x8c\xe8\xbf\x90\xe8\xa1\x8ctensorflow\xe7\xa8\x8b\xe5\xba\x8f\n def run_tf_program_with_json_config(program):\n   if program == \'train\':\n     script = \'solver.py\'\n@@ -116,6 +117,7 @@ def upload_logs():\n \n \n if __name__ == \'__main__\':\n+  # \xe8\xae\xad\xe7\xbb\x83\xe5\x91\xbd\xe4\xbb\xa4\xef\xbc\x9apython manage.py train exp/sgd finetune_ic15\n   if len(sys.argv) < 2:\n     print(\'Usage: python3 manage.py <function-name>\')\n   else:\ndiff --git a/seglink/cpp/CMakeLists.txt b/seglink/cpp/CMakeLists.txt\nindex e772826..fa20c0a 100644\n--- a/seglink/cpp/CMakeLists.txt\n+++ b/seglink/cpp/CMakeLists.txt\n@@ -1,19 +1,34 @@\n CMAKE_MINIMUM_REQUIRED(VERSION 2.8)\n PROJECT(seglink)\n \n+#EXECUTE_PROCESS(COMMAND python3 -c "import os; os.environ[\'TF_CPP_MIN_LOG_LEVEL\']=\'3\';import tensorflow as tf; print(tf.sysconfig.get_lib())"\n+#                OUTPUT_VARIABLE TF_LIB)\n+\n+#MESSAGE(STATUS "Found TF_LIB: " ${TF_LIB})\n+#-L$TF_LIB -ltensorflow_framework\n # compiler flags\n SET(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -std=c++11 ${OpenMP_CXX_FLAGS} -Wall -fPIC -D_GLIBCXX_USE_CXX11_ABI=0")\n-\n # TensorFlow dependencies\n-EXECUTE_PROCESS(COMMAND python3 -c "import os; os.environ[\'TF_CPP_MIN_LOG_LEVEL\']=\'3\'; import tensorflow as tf; print(tf.sysconfig.get_include())"\n+EXECUTE_PROCESS(COMMAND python3 -c "import os; os.environ[\'TF_CPP_MIN_LOG_LEVEL\']=\'3\';import tensorflow as tf; print(tf.sysconfig.get_include())"\n                 OUTPUT_VARIABLE TF_INC)\n+\n+\n+\n MESSAGE(STATUS "Found TF_INC: " ${TF_INC})\n \n+\n+\n # boost\n # find_package (Boost REQUIRED COMPONENTS graph)\n \n+#TF_CFLAGS=( $(python -c \'import tensorflow as tf; print(" ".join(tf.sysconfig.get_compile_flags()))\') )\n+#TF_LFLAGS=( $(python -c \'import tensorflow as tf; print(" ".join(tf.sysconfig.get_link_flags()))\') )\n+\n+\n # target\n+#link_directories(${TF_LIB})\n INCLUDE_DIRECTORIES(${TF_INC})\n+#LINK_LIBRARIES(${TF_LIB})\n ADD_LIBRARY(seglink SHARED \n   utilities.h\n   sample_crop_bbox_op.cc\ndiff --git a/seglink/cpp/combine_segments_op.cc b/seglink/cpp/combine_segments_op.cc\nindex afdbb68..ffe6ea8 100644\n--- a/seglink/cpp/combine_segments_op.cc\n+++ b/seglink/cpp/combine_segments_op.cc\n@@ -79,7 +79,8 @@ class CombineSegmentsOp : public OpKernel {\n     for (int i = 0; i < batch_size; ++i) {\n       const auto& rboxes = batch_combined_rboxes[i];\n       combined_counts_tensor(i) = rboxes.size();\n-      for (int j = 0; j < rboxes.size(); ++j) {\n+      int rboxes_size = rboxes.size();\n+      for (int j = 0; j < rboxes_size; ++j) {\n         for (int k = 0; k < rbox_dim_; ++k) {\n           combined_rboxes_tensor(i, j, k) = rboxes[j][k];\n         }\ndiff --git a/seglink/cpp/decode_segments_links_op.cc b/seglink/cpp/decode_segments_links_op.cc\nindex 2fb4939..bda58f3 100644\n--- a/seglink/cpp/decode_segments_links_op.cc\n+++ b/seglink/cpp/decode_segments_links_op.cc\n@@ -139,7 +139,8 @@ class DecodeSegmentsLinksOp : public OpKernel {\n     auto group_indices_tensor = output_group_indices->tensor<int, 2>();\n     auto counts_tensor = output_counts->tensor<int, 1>();\n     for (int i = 0; i < batch_size; i++) {\n-      for (int j = 0; j < batch_segments[i].size(); j++) {\n+    int batch_segments_size = batch_segments[i].size();\n+      for (int j = 0; j < batch_segments_size; j++) {\n         for (int k = 0; k < seg_dim_; k++) {\n           segments_tensor(i,j,k) = batch_segments[i][j][k];\n         }\ndiff --git a/seglink/data.py b/seglink/data.py\nindex 50ab821..5d882ef 100644\n--- a/seglink/data.py\n+++ b/seglink/data.py\n@@ -4,6 +4,7 @@ import numpy as np\n import ops\n \n FLAGS = tf.app.flags.FLAGS\n+#   IOU \xe5\xb9\xb6\xe4\xba\xa4\xe6\xaf\x94\n tf.app.flags.DEFINE_string(\'sampling_overlap_mode\', \'coverage\', \'Sampling based on jaccard / coverage\')\n tf.app.flags.DEFINE_string(\'image_channel_order\', \'BGR\', \'Order of input image channels\')\n tf.app.flags.DEFINE_integer(\'max_num_gt\', 300, \'Max number of groundtruths in one example, used for determining padding length\')\ndiff --git a/seglink/evaluate.py b/seglink/evaluate.py\nindex 23a3301..67f153f 100644\n--- a/seglink/evaluate.py\n+++ b/seglink/evaluate.py\n@@ -226,8 +226,7 @@ def postprocess_and_write_results_ic15(all_batches, result_dir):\n \n \n def postprocess_and_write_results_ic13(all_results):\n-  raise NotImplementedError(\'This function needs revision\')\n-  \n+  # raise NotImplementedError(\'This function needs revision\')\n   for j in range(batch_size):\n     # convert detection results\n     rboxes = sess_outputs[\'combined_rboxes\'][j]\ndiff --git a/seglink/fpn.py b/seglink/fpn.py\nindex e69de29..8eeaaf3 100644\n--- a/seglink/fpn.py\n+++ b/seglink/fpn.py\n@@ -0,0 +1,43 @@\n+# \xe5\x9c\xa8segment link\xe4\xb8\x8a\xe6\xb7\xbb\xe5\x8a\xa0fpn\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe5\xaf\xb9\xe5\xb0\x8f\xe6\x96\x87\xe6\x9c\xac\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe6\x8f\x90\xe5\x8f\x96\xe6\x9b\xb4\xe5\x8a\xa0\xe5\xa5\xbd\n+import tensorflow as tf\n+import tensorflow.contrib.slim as slim\n+def build_feature_pyramid(self):\n+    \'\'\'\n+    reference: https://github.com/CharlesShang/FastMaskRCNN\n+    build P2, P3, P4, P5\n+    :return: multi-scale feature map\n+    \'\'\'\n+\n+    feature_pyramid = {}\n+    with tf.variable_scope(\'build_feature_pyramid\'):\n+        with slim.arg_scope([slim.conv2d], weights_regularizer=slim.l2_regularizer(self.rpn_weight_decay)):\n+            # \xe9\xa6\x96\xe5\x85\x88\xe4\xbd\xbf\xe7\x94\xa8\n+            feature_pyramid[\'P5\'] = slim.conv2d(self.feature_maps_dict[\'C5\'],\n+                                                num_outputs=256,\n+                                                kernel_size=[1, 1],\n+                                                stride=1,\n+                                                scope=\'build_P5\')\n+\n+            feature_pyramid[\'P6\'] = slim.max_pool2d(feature_pyramid[\'P5\'],\n+                                                    kernel_size=[2, 2], stride=2, scope=\'build_P6\')\n+\n+            # \xe4\xb8\x8b\xe9\x87\x87\xe6\xa0\xb7\xef\xbc\x8c\xe4\xbb\x8econv11\xe5\x88\xb0conv7\xe5\xb1\x82\n+            # P6 is down sample of P5\n+            for layer in range(6, 1, -1):\n+                p, c = feature_pyramid[\'P\' + str(layer + 1)], self.feature_maps_dict[\'C\' + str(layer)]\n+                up_sample_shape = tf.shape(c)\n+                # \xe6\x9c\x80\xe8\xbf\x91\xe9\x82\xbb\xe6\x8f\x92\xe5\x80\xbc\n+                up_sample = tf.image.resize_nearest_neighbor(p, [up_sample_shape[1], up_sample_shape[2]],\n+                                                             name=\'build_P%d/up_sample_nearest_neighbor\' % layer)\n+                # \xe9\x99\x8d\xe7\xbb\xb4\xef\xbc\x8ckernel size = 1*1\n+                c = slim.conv2d(c, num_outputs=256, kernel_size=[1, 1], stride=1,\n+                                scope=\'build_P%d/reduce_dimension\' % layer)\n+                # \xe7\x9b\xb4\xe6\x8e\xa5\xe5\xb0\x86\xe4\xb8\x8a\xe9\x87\x87\xe6\xa0\xb7\xe7\x9a\x84\xe5\x80\xbc\xe5\x92\x8cc\xe7\x9a\x84\xe5\x80\xbc\xe5\x81\x9aconcate\n+                p = up_sample + c\n+                # 3\xc3\x973\xe5\x8d\xb7\xe7\xa7\xaf\n+                p = slim.conv2d(p, 256, kernel_size=[3, 3], stride=1,\n+                                padding=\'SAME\', scope=\'build_P%d/avoid_aliasing\' % layer)\n+\n+                feature_pyramid[\'P\' + str(layer)] = p\n+\n+    return feature_pyramid\n\\ No newline at end of file\ndiff --git a/seglink/model.py b/seglink/model.py\nindex 3581ae4..5b7a0ba 100644\n--- a/seglink/model.py\n+++ b/seglink/model.py\n@@ -37,6 +37,7 @@ class SegLinkDetector():\n     self.anchor_sizes = [11.84210526, 23.68421053, 45., 90., 150., 285.]\n     logging.info(\'Anchor sizes: {}\'.format(self.anchor_sizes))\n \n+  # \xe5\x88\x86\xe7\xb1\xbb\xe6\xa3\x80\xe6\xb5\x8b\xe5\x99\xa8\n   def _detection_classifier(self, maps, ksize, cross_links=False, scope=None):\n     """\n     Create a SegLink detection classifier on a feature layer\ndiff --git a/seglink/model_cnn_ckpt.py b/seglink/model_cnn_ckpt.py\nindex fe852a6..15e66e2 100644\n--- a/seglink/model_cnn_ckpt.py\n+++ b/seglink/model_cnn_ckpt.py\n@@ -6,7 +6,7 @@ import ops\n import utils\n \n FLAGS = tf.app.flags.FLAGS\n-\n+tf.app.flags.DEFINE_string("weight_init_method", "xavier", "")\n \n class SsdVgg16():\n   def __init__(self):\ndiff --git a/seglink/ops.py b/seglink/ops.py\nindex 2c78319..7bf51ef 100644\n--- a/seglink/ops.py\n+++ b/seglink/ops.py\n@@ -15,9 +15,11 @@ def load_oplib(lib_name):\n   """\n   # use absolute path so that ops.py can be called from other directory\n   lib_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), \'lib{0}.so\'.format(lib_name))\n+  print(lib_path)\n   # duplicate library with a random new name so that\n   # a running program will not be interrupted when the original library is updated\n   lib_copy_path = \'/tmp/lib{0}_{1}.so\'.format(str(uuid.uuid4())[:8], LIB_NAME)\n+  print(lib_copy_path)\n   shutil.copyfile(lib_path, lib_copy_path)\n   oplib = tf.load_op_library(lib_copy_path)\n   return oplib\n@@ -94,7 +96,7 @@ def conv_relu(*args, **kwargs):\n     kwargs[\'scope\'] = \'conv_relu\'\n   return conv2d(*args, **kwargs)\n \n-\n+# \xe7\xa9\xba\xe6\xb4\x9e\xe5\x8d\xb7\xe7\xa7\xaf\n def atrous_conv2d(x, n_in, n_out, ksize, dilation, padding=\'SAME\',\n                   weight_init=None, bias=True,\n                   relu=False, scope=None, **kwargs):\n@@ -102,6 +104,7 @@ def atrous_conv2d(x, n_in, n_out, ksize, dilation, padding=\'SAME\',\n   trainable = kwargs.get(\'trainable\', True)\n   with tf.variable_scope(scope or \'atrous_conv2d\'):\n     # atrous convolution\n+    # \xe7\xa9\xba\xe6\xb4\x9e\xe5\x8d\xb7\xe7\xa7\xaf\n     kernel = _nn_variable(\'weight\', [ksize,ksize,n_in,n_out], weight_init,\n                           collection=\'weights\' if trainable else None,\n                           **kwargs)\n@@ -117,13 +120,13 @@ def atrous_conv2d(x, n_in, n_out, ksize, dilation, padding=\'SAME\',\n       y = tf.nn.relu(y)\n     return y\n \n-\n+# \xe5\xb9\xb3\xe5\x9d\x87\xe6\xb1\xa0\xe5\x8c\x96\n def avg_pool(x, ksize, stride, padding=\'SAME\', scope=None):\n   with tf.variable_scope(scope or \'avg_pool\'):\n     y = tf.nn.avg_pool(x, [1,ksize,ksize,1], [1,stride,stride,1], padding)\n   return y\n \n-\n+# \xe6\x9c\x80\xe5\xa4\xa7\xe6\xb1\xa0\xe5\x8c\x96\n def max_pool(x, ksize, stride, padding=\'SAME\', scope=None):\n   with tf.variable_scope(scope or \'max_pool\'):\n     y = tf.nn.max_pool(x, [1,ksize,ksize,1], [1,stride,stride,1], padding)\ndiff --git a/seglink/preprocess_data.py b/seglink/preprocess_data.py\nindex e69de29..e9e4789 100644\n--- a/seglink/preprocess_data.py\n+++ b/seglink/preprocess_data.py\n@@ -0,0 +1,124 @@\n+import cv2\n+from math import *\n+import math\n+import numpy as np\n+import os\n+import glob\n+\n+\'\'\'\xe6\x97\x8b\xe8\xbd\xac\xe5\x9b\xbe\xe5\x83\x8f\xe5\xb9\xb6\xe5\x89\xaa\xe8\xa3\x81\'\'\'\n+\n+def rotate(\n+        img,  # \xe5\x9b\xbe\xe7\x89\x87\n+        pt1, pt2, pt3, pt4,  # \xe5\x9b\x9b\xe7\x82\xb9\xe5\x9d\x90\xe6\xa0\x87\n+        NewimageName  # \xe8\xbe\x93\xe5\x87\xba\xe5\x9b\xbe\xe7\x89\x87\xe8\xb7\xaf\xe5\xbe\x84\n+):\n+    withRect = math.sqrt((pt4[0] - pt1[0]) ** 2 + (pt4[1] - pt1[1]) ** 2)  # \xe7\x9f\xa9\xe5\xbd\xa2\xe6\xa1\x86\xe7\x9a\x84\xe5\xae\xbd\xe5\xba\xa6\n+    #    heightRect = math.sqrt((pt1[0] - pt2[0]) ** 2 + (pt1[1] - pt2[1]) **2)\n+    if (withRect != 0):\n+        angle = acos((pt4[0] - pt1[0]) / withRect) * (180 / math.pi)  # \xe7\x9f\xa9\xe5\xbd\xa2\xe6\xa1\x86\xe6\x97\x8b\xe8\xbd\xac\xe8\xa7\x92\xe5\xba\xa6\n+\n+        if pt4[1] < pt1[1]:\n+            angle = -angle\n+\n+        height = img.shape[0]  # \xe5\x8e\x9f\xe5\xa7\x8b\xe5\x9b\xbe\xe5\x83\x8f\xe9\xab\x98\xe5\xba\xa6\n+        width = img.shape[1]  # \xe5\x8e\x9f\xe5\xa7\x8b\xe5\x9b\xbe\xe5\x83\x8f\xe5\xae\xbd\xe5\xba\xa6\n+        rotateMat = cv2.getRotationMatrix2D((width / 2, height / 2), angle, 1)  # \xe6\x8c\x89angle\xe8\xa7\x92\xe5\xba\xa6\xe6\x97\x8b\xe8\xbd\xac\xe5\x9b\xbe\xe5\x83\x8f\n+        heightNew = int(width * fabs(sin(radians(angle))) + height * fabs(cos(radians(angle))))\n+        widthNew = int(height * fabs(sin(radians(angle))) + width * fabs(cos(radians(angle))))\n+\n+        rotateMat[0, 2] += (widthNew - width) / 2\n+        rotateMat[1, 2] += (heightNew - height) / 2\n+        imgRotation = cv2.warpAffine(img, rotateMat, (widthNew, heightNew), borderValue=(255, 255, 255))\n+\n+        # \xe6\x97\x8b\xe8\xbd\xac\xe5\x90\x8e\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe5\x9b\x9b\xe7\x82\xb9\xe5\x9d\x90\xe6\xa0\x87\n+        [[pt1[0]], [pt1[1]]] = np.dot(rotateMat, np.array([[pt1[0]], [pt1[1]], [1]]))\n+        [[pt3[0]], [pt3[1]]] = np.dot(rotateMat, np.array([[pt3[0]], [pt3[1]], [1]]))\n+        [[pt2[0]], [pt2[1]]] = np.dot(rotateMat, np.array([[pt2[0]], [pt2[1]], [1]]))\n+        [[pt4[0]], [pt4[1]]] = np.dot(rotateMat, np.array([[pt4[0]], [pt4[1]], [1]]))\n+\n+        # \xe5\xa4\x84\xe7\x90\x86\xe5\x8f\x8d\xe8\xbd\xac\xe7\x9a\x84\xe6\x83\x85\xe5\x86\xb5\n+        if pt2[1] > pt4[1]:\n+            pt2[1], pt4[1] = pt4[1], pt2[1]\n+        if pt1[0] > pt3[0]:\n+            pt1[0], pt3[0] = pt3[0], pt1[0]\n+\n+        imgOut = imgRotation[int(pt2[1]):int(pt4[1]), int(pt1[0]):int(pt3[0])]\n+        cv2.imwrite(NewimageName, imgOut)  # \xe4\xbf\x9d\xe5\xad\x98\xe5\xbe\x97\xe5\x88\xb0\xe7\x9a\x84\xe6\x97\x8b\xe8\xbd\xac\xe5\x90\x8e\xe7\x9a\x84\xe7\x9f\xa9\xe5\xbd\xa2\xe6\xa1\x86\n+        return imgRotation  # rotated image\n+\n+cv2.resize()\n+\n+# \xe8\xaf\xbb\xe5\x8f\x96\xe5\x9b\x9b\xe7\x82\xb9\xe5\x9d\x90\xe6\xa0\x87\n+def ReadTxt(directory, last):\n+    global Newpathofimage, Newpathoftxt, allpic, nowimage, nowtxt, nowline, invalidimg\n+    SetofimageName = glob.glob(\'*.jpg\')  # \xe8\x8e\xb7\xe5\x8f\x96\xe5\xbd\x93\xe5\x89\x8d\xe8\xb7\xaf\xe5\xbe\x84\xe4\xb8\x8b\xe6\x89\x80\xe6\x9c\x89jpg\xe6\xa0\xbc\xe5\xbc\x8f\xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xe5\x88\xb0list\xe4\xb8\xad\n+    Numofimage = len(SetofimageName)\n+    for j in range(Numofimage):\n+        print(\'\xe5\xa4\x84\xe7\x90\x86\xe5\x9b\xbe\xe7\x89\x87:\' + str(j))\n+        imageTxt = directory + SetofimageName[j][:-4] + last  # txt\xe8\xb7\xaf\xe5\xbe\x84\n+        imageName = SetofimageName[j]\n+        nowimage = imageName\n+        nowtxt = imageTxt\n+        nowline = 0\n+        imgSrc = cv2.imread(imageName)\n+        if (imgSrc is None):\n+            invalidimg.append(nowimage)\n+        else:\n+            F = open(imageTxt, \'rb\')  # \xe4\xbb\xa5\xe4\xba\x8c\xe8\xbf\x9b\xe5\x88\xb6\xe6\xa8\xa1\xe5\xbc\x8f\xe6\x89\x93\xe5\xbc\x80\xe7\x9b\xae\xe6\xa0\x87txt\xe6\x96\x87\xe4\xbb\xb6\n+            lines = F.readlines()  # \xe9\x80\x90\xe8\xa1\x8c\xe8\xaf\xbb\xe5\x85\xa5\xe5\x86\x85\xe5\xae\xb9\n+            length = len(lines)\n+            s = 0  # \xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe7\x89\x87\xe7\xbc\x96\xe5\x8f\xb7\xef\xbc\x8c\xe5\xaf\xb9\xe5\xba\x94\xe6\x96\x87\xe6\x9c\xac\xe6\x8f\x8f\xe8\xbf\xb0\n+            for i in range(length):\n+                lines[i] = str(lines[i], encoding="utf-8")  # \xe4\xbb\x8ebytes\xe8\xbd\xac\xe4\xb8\xbastr\xe6\xa0\xbc\xe5\xbc\x8f\n+                des = lines[i].split(\',\')[-1:]\n+                nowline = i\n+                if ((des != [\'###\\n\']) and (des != [\'###\'])):\n+                    s = s + 1\n+                    allpic += 1\n+                    # \xe4\xbf\x9d\xe5\xad\x98\xe6\x96\xb0\xe5\x9b\xbe\xe7\x89\x87/txt\xe6\xa0\xbc\xe5\xbc\x8f\xe4\xb8\xba"\xe5\x8e\x9f\xe5\x90\x8d\xe5\xad\x97+\xe7\xbc\x96\xe5\x8f\xb7+.jpg/.txt"\n+                    NewimageName = Newpathofimage + imageName[:-3] + str(s) + \'.jpg\'\n+                    NewtxtName = Newpathoftxt + imageName[:-3] + str(s) + \'.txt\'\n+                    # \xe5\x86\x99\xe5\x85\xa5\xe6\x96\xb0TXT\xe6\x96\x87\xe4\xbb\xb6\n+                    if (s == length):\n+                        des = str(des)[2:-2]\n+                    else:\n+                        des = str(des)[2:-4]\n+                    file = open(NewtxtName, \'w\')  # \xe6\x89\x93\xe5\xbc\x80or\xe5\x88\x9b\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe6\x96\xb0\xe7\x9a\x84txt\xe6\x96\x87\xe4\xbb\xb6\n+                    file.write(des)  # \xe5\x86\x99\xe5\x85\xa5\xe5\x86\x85\xe5\xae\xb9\xe4\xbf\xa1\xe6\x81\xaf\n+                    file.close()\n+                    # str\xe8\xbd\xacfloat\n+                    pt1 = list(map(float, lines[i].split(\',\')[:2]))\n+                    pt2 = list(map(float, lines[i].split(\',\')[2:4]))\n+                    pt3 = list(map(float, lines[i].split(\',\')[4:6]))\n+                    pt4 = list(map(float, lines[i].split(\',\')[6:8]))\n+                    # float\xe8\xbd\xacint\n+                    pt1 = list(map(int, pt1))\n+                    pt2 = list(map(int, pt2))\n+                    pt4 = list(map(int, pt4))\n+                    pt3 = list(map(int, pt3))\n+                    rotate(imgSrc, pt1, pt2, pt3, pt4, NewimageName)\n+\n+\n+if __name__ == "__main__":\n+    img_path = "/apps/yinhong/heiren/data/all_2/\xe4\xba\x91\xe5\x8d\x97\xe7\x99\xbd\xe8\x8d\xaf\xe5\x86\xac\xe9\x9d\x92\xe9\xa6\x99\xe5\x9e\x8b\xe7\x89\x99\xe8\x86\x8f135g_7006/heiren_3d6961324e30aa4ca57ffd81700e62f2#71.jpg"\n+    image = cv2.imread(img_path)\n+    img = cv2.flip(image,-1)\n+    cv2.imshow(\'result.jpg\', img)\n+    cv2.waitKey(0)\n+    cv2.destroyAllWindows()\n+    cv2.waitKey(1)\n+    cv2.waitKey(1)\n+    cv2.waitKey(1)\n+    cv2.waitKey(1)\n+    # Newpathofimage = \'d:\\\\competition\\\\[update] ICPR_text_train_part1_20180316\\\\newimage\\\\\'\n+    # Newpathoftxt = \'d:\\\\competition\\\\[update] ICPR_text_train_part1_20180316\\\\newtxt\\\\\'\n+    # allpic = 0\n+    # nowimage = \'\'\n+    # nowtxt = \'\'\n+    # nowline = 0\n+    # invalidimg = []\n+    # os.chdir(\'d:\\\\competition\\\\[update] ICPR_text_train_part1_20180316\\\\image_1000\')  # \xe4\xbf\xae\xe6\x94\xb9\xe9\xbb\x98\xe8\xae\xa4\xe8\xb7\xaf\xe5\xbe\x84\n+    # retval = os.getcwd()  # \xe8\x8e\xb7\xe5\x8f\x96\xe5\xbd\x93\xe5\x89\x8d\xe8\xb7\xaf\xe5\xbe\x84\n+    # directory = \'d:\\\\competition\\\\[update] ICPR_text_train_part1_20180316\\\\txt_1000\\\\\'  # TXT\xe6\x96\x87\xe4\xbb\xb6\xe8\xb7\xaf\xe5\xbe\x84\n+    # last = \'.txt\'\n+    # ReadTxt(directory, last)\n\\ No newline at end of file\ndiff --git a/seglink/solver.py b/seglink/solver.py\nindex b4500c8..7f7a923 100644\n--- a/seglink/solver.py\n+++ b/seglink/solver.py\n@@ -1,3 +1,4 @@\n+# coding=utf-8\n import os\n import sys\n import re\n@@ -19,6 +20,7 @@ tf.app.flags.DEFINE_string(\'log_dir\', \'\', \'Directory for saving checkpoints and\n tf.app.flags.DEFINE_string(\'log_prefix\', \'\', \'Log file name prefix\')\n # training\n tf.app.flags.DEFINE_string(\'resume\', \'vgg16\', \'Training from loading VGG16 parameters ("vgg16"), resume a checkpoint ("resume"), or finetune a pretrained model ("finetune")\')\n+# tf.app.flags.DEFINE_string(\'vgg16_model\', \'../data/VGG_ILSVRC_16_layers_ssd.ckpt\', \'The pretrained VGG16 model checkpoint\')\n tf.app.flags.DEFINE_string(\'vgg16_model\', \'../data/VGG_ILSVRC_16_layers_ssd.ckpt\', \'The pretrained VGG16 model checkpoint\')\n tf.app.flags.DEFINE_string(\'finetune_model\', \'\', \'Finetuning model path\')\n tf.app.flags.DEFINE_string(\'train_datasets\', \'\', \'Training datasets file names separated by semicolons\')\n@@ -26,12 +28,16 @@ tf.app.flags.DEFINE_string(\'weight_init_method\', \'xavier\', \'Weight initializatio\n tf.app.flags.DEFINE_integer(\'train_batch_size\', 32, \'Training batch size\')\n tf.app.flags.DEFINE_integer(\'n_gpu\', 1, \'Number of GPUs used in training\')\n tf.app.flags.DEFINE_float(\'hard_neg_ratio\', 3.0, \'Ratio of hard negatives to positives\')\n+# \xe4\xb8\x8d\xe8\xbf\x9b\xe8\xa1\x8c\xe9\x9a\x8f\xe6\x9c\xba\xe5\x89\xaa\xe8\xa3\x81\xef\xbc\x8c\xe7\x9b\xb4\xe6\x8e\xa5\xe4\xbd\xbf\xe7\x94\xa8\xe5\x8e\x9f\xe5\x9b\xbe\n tf.app.flags.DEFINE_integer(\'no_random_crop\', 0, \'In data augmentation, do not crop image, i.e. use full images\')\n # optimizer\n tf.app.flags.DEFINE_string(\'optimizer\', \'sgd\', \'Optimization algorithm\')\n+# \xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\n tf.app.flags.DEFINE_float(\'base_lr\', 1e-3, \'Base learning rate\')\n+# \xe5\x8a\xa8\xe9\x87\x8f\n tf.app.flags.DEFINE_float(\'momentum\', 0.9, \'SGD momentum\')\n tf.app.flags.DEFINE_float(\'weight_decay\', 5e-4, \'SGD weight decay\')\n+# \xe8\xae\xbe\xe7\xbd\xae\xe8\xbf\xad\xe4\xbb\xa3\xe6\xac\xa1\xe6\x95\xb0\n tf.app.flags.DEFINE_integer(\'max_steps\', 60000, \'Maximum number of iterations.\')\n tf.app.flags.DEFINE_string(\'lr_policy\', \'staircase\', \'Learning rate decaying policy\')\n tf.app.flags.DEFINE_string(\'lr_breakpoints\', \'\', \'Comma-separated breakpoints of learning rate decay\')\n@@ -44,9 +50,10 @@ tf.app.flags.DEFINE_integer(\'brief_summary_period\', 10, \'Period for brief summar\n tf.app.flags.DEFINE_integer(\'detailed_summary_period\', 200, \'Period for detailed summaries\')\n tf.app.flags.DEFINE_integer(\'checkpoint_period\', 5000, \'Period for saving checkpoints\')\n \n-\n+# \xe8\xae\xad\xe7\xbb\x83\xe4\xbb\xa3\xe7\xa0\x81\n class Solver:\n   def __init__(self):\n+    # \xe5\xae\x9a\xe4\xb9\x89seglink\xe6\xa3\x80\xe6\xb5\x8b\xe5\x99\xa8\n     self.detector = model.SegLinkDetector()\n \n     # global TF variables\n@@ -54,7 +61,7 @@ class Solver:\n       self.global_step = tf.Variable(0, trainable=False, name=\'global_step\', dtype=tf.int64)\n       tf.summary.scalar(\'global_step\', self.global_step, collections=[\'brief\'])\n \n-    # setup training graphs and summaries\n+    # setup training graphs and summaries\xef\xbc\x8c\xe5\xa4\x9agpu\xe8\xae\xad\xe7\xbb\x83\n     self._setup_train_net_multigpu()\n \n     # if true the training process will be terminated in the next iteration\n@@ -63,13 +70,13 @@ class Solver:\n   def _tower_loss(self, train_batch):\n     images = train_batch[\'image\']\n \n-    # model and loss function\n+    # model and loss function\xef\xbc\x8c\xe6\x9e\x84\xe5\xbb\xba\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x92\x8c\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\n     outputs = self.detector.build_model(images)\n     image_size = tf.shape(images)[1:]\n     tower_loss = self.detector.build_loss(outputs, train_batch[\'rboxes\'],\n                                           train_batch[\'count\'], image_size)\n     return tower_loss\n-\n+  # \xe5\xae\x9a\xe4\xb9\x89\xe5\xb9\xb3\xe5\x9d\x87\xe6\xa2\xaf\xe5\xba\xa6\n   def _average_gradients(self, tower_grads):\n     average_grads = []\n     for grad_and_vars in zip(*tower_grads):\n@@ -191,26 +198,36 @@ class Solver:\n   def train_and_eval(self):\n     # register handler for ctrl-c\n     self._register_signal_handler()\n-\n+    # tf.ConfigProto()\xef\xbc\x9atensorflow\xe7\x94\xa8\xe4\xba\x8e\xe5\x8f\x82\xe6\x95\xb0\xe7\x9a\x84\xe9\x85\x8d\xe7\xbd\xae\n+    # log_device_placement=True : \xe6\x98\xaf\xe5\x90\xa6\xe6\x89\x93\xe5\x8d\xb0\xe8\xae\xbe\xe5\xa4\x87\xe5\x88\x86\xe9\x85\x8d\xe6\x97\xa5\xe5\xbf\x97\n+    # allow_soft_placement=True \xef\xbc\x9a \xe5\xa6\x82\xe6\x9e\x9c\xe4\xbd\xa0\xe6\x8c\x87\xe5\xae\x9a\xe7\x9a\x84\xe8\xae\xbe\xe5\xa4\x87\xe4\xb8\x8d\xe5\xad\x98\xe5\x9c\xa8\xef\xbc\x8c\xe5\x85\x81\xe8\xae\xb8TF\xe8\x87\xaa\xe5\x8a\xa8\xe5\x88\x86\xe9\x85\x8d\xe8\xae\xbe\xe5\xa4\x87\n     sess_config = tf.ConfigProto(log_device_placement=False,\n                                  allow_soft_placement=True)\n     with tf.Session(config=sess_config) as sess:\n       # create summary writer and saver\n       summary_writer = tf.summary.FileWriter(FLAGS.log_dir, graph=sess.graph)\n+      # \xe6\xa8\xa1\xe5\x9e\x8b\xe4\xbf\x9d\xe5\xad\x98\xef\xbc\x8c\xe5\x85\x88\xe8\xa6\x81\xe5\x88\x9b\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaaSaver\xe5\xaf\xb9\xe8\xb1\xa1\xe3\x80\x82max_to_keep \xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe6\x98\xaf\xe7\x94\xa8\xe6\x9d\xa5\xe8\xae\xbe\xe7\xbd\xae\xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe4\xb8\xaa\xe6\x95\xb0\xef\xbc\x8c\xe9\xbb\x98\xe8\xae\xa4\xe4\xb8\xba5\xef\xbc\x8c\n       saver = tf.train.Saver(max_to_keep=20)\n \n       # resume training, load pretrained model, or start training from scratch\n       if FLAGS.resume == \'resume\':\n+        # \xe4\xbb\x8e\xe6\x9c\x80\xe8\xbf\x91\xe7\x9a\x84checkpoint\xe4\xb8\x8a\xe5\x8a\xa0\xe8\xbd\xbd\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xae\xad\xe7\xbb\x83\xef\xbc\x8c\xe5\x8d\xb3\xe6\x96\xad\xe7\x82\xb9\xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xad\xe7\xbb\x83\n         latest_ckpt_path = tf.train.latest_checkpoint(FLAGS.log_dir)\n+        print("========================")\n+        print(latest_ckpt_path)\n         if latest_ckpt_path is None:\n           logging.error(\'Failed to find the latest checkpoint from {}\'.format(FLAGS.log_dir))\n           sys.exit(1)\n         else:\n           model_loader = tf.train.Saver()\n+\n           model_loader.restore(sess, latest_ckpt_path)\n+          # model_loader.restore(sess, "../data/VGG_ILSVRC_16_layers_ssd.ckpt")\n           logging.info(\'Resuming checkpoint %s\' % latest_ckpt_path)\n+      # \xe4\xbb\x8e\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xb8\xadfinetune\n       elif FLAGS.resume == \'finetune\':\n         ckpt_path = FLAGS.finetune_model\n+        print(ckpt_path)\n         model_loader = tf.train.Saver()\n         model_loader.restore(sess, ckpt_path)\n         tf.assign(self.global_step, 0).eval() # reset global_step\n@@ -218,6 +235,8 @@ class Solver:\n       elif FLAGS.resume == \'vgg16\':\n         logging.info(\'Initializing model\')\n         sess.run(tf.global_variables_initializer())\n+        # \xe4\xbb\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe5\x88\x97\xe8\xa1\xa8\xe4\xb8\xad\xe5\x8f\x96\xe5\x87\xba\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe5\x85\x83\xe7\xb4\xa0\xef\xbc\x8ctf.get_collection().GLOBAL_VARIABLES: \xe8\xaf\xa5collection\xe9\xbb\x98\xe8\xae\xa4\xe5\x8a\xa0\xe5\x85\xa5\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84Variable\xe5\xaf\xb9\xe8\xb1\xa1\xef\xbc\x8c\xe5\xb9\xb6\xe4\xb8\x94\xe5\x9c\xa8\xe5\x88\x86\xe5\xb8\x83\xe5\xbc\x8f\xe7\x8e\xaf\xe5\xa2\x83\xe4\xb8\xad\xe5\x85\xb1\xe4\xba\xab\xe3\x80\x82\n+        # \xe4\xb8\x80\xe8\x88\xac\xe6\x9d\xa5\xe8\xaf\xb4\xef\xbc\x8cTRAINABLE_VARIABLES\xe5\x8c\x85\xe5\x90\xab\xe5\x9c\xa8MODEL_VARIABLES\xe4\xb8\xad\xef\xbc\x8cMODEL_VARIABLES\xe5\x8c\x85\xe5\x90\xab\xe5\x9c\xa8GLOBAL_VARIABLES\xe4\xb8\xad\xe3\x80\x82\n         vgg16_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\'ssd/vgg16/\')\n         pretrained_loader = tf.train.Saver(var_list=vgg16_vars)\n         pretrained_loader.restore(sess, FLAGS.vgg16_model)\n@@ -310,7 +329,7 @@ class Solver:\n   def _handle_ctrl_c(self, signal, frame):\n     logging.info(\'Ctrl-C pressed, terminating training process\')\n     self.should_stop = True\n-\n+  # \xe8\xae\xbe\xe7\xbd\xaectrl+c\xe4\xb8\xad\xe6\x96\xad\xe8\xae\xad\xe7\xbb\x83\n   def _register_signal_handler(self):\n     import signal\n     signal.signal(signal.SIGINT, self._handle_ctrl_c)\ndiff --git a/seglink/unit_tests.py b/seglink/unit_tests.py\nindex 66bd209..d963e9a 100644\n--- a/seglink/unit_tests.py\n+++ b/seglink/unit_tests.py\n@@ -246,7 +246,7 @@ def test_encode_decode_real_data():\n     sess_outputs = sess.run(fetches)\n \n     fig = plt.figure()\n-    for i in xrange(batch_size):\n+    for i in range(batch_size):\n       fig.clear()\n       for j, det_layer in enumerate(det_layers):\n         ax = fig.add_subplot(2, 3, j+1)\n@@ -330,12 +330,12 @@ def test_data_loading_and_preprocess():\n   with tf.Session() as sess:\n     sess.run(tf.initialize_all_variables())\n     tf.train.start_queue_runners(sess=sess)\n-    for i in xrange(n_batches):\n+    for i in range(n_batches):\n       fetches = {\'images\': batches[\'image\'],\n                  \'gt_rboxes\': batches[\'rboxes\'],\n                  \'gt_counts\': batches[\'count\']}\n       sess_outputs = sess.run(fetches)\n-      for j in xrange(batch_size):\n+      for j in range(batch_size):\n         save_path = os.path.join(save_dir, \'%04d_%d.jpg\' % (i, j))\n         gt_count = sess_outputs[\'gt_counts\'][j]\n         _visualize_example(save_path,\ndiff --git a/seglink/utils.py b/seglink/utils.py\nindex 81ba8ee..1b64866 100644\n--- a/seglink/utils.py\n+++ b/seglink/utils.py\n@@ -86,6 +86,7 @@ def print_tensor_summary(tensor, tag=None, n_print=21):\n \n \n def mkdir_if_not_exist(directory):\n+  print(directory)\n   if not os.path.exists(directory):\n     os.makedirs(directory)\n \ndiff --git a/tool/convert_caffe_model/convert_caffemodel_to_ckpt.py b/tool/convert_caffe_model/convert_caffemodel_to_ckpt.py\nindex 24c9297..7390388 100644\n--- a/tool/convert_caffe_model/convert_caffemodel_to_ckpt.py\n+++ b/tool/convert_caffe_model/convert_caffemodel_to_ckpt.py\n@@ -6,12 +6,16 @@ import argparse\n \n parentdir = os.path.dirname(__file__)\n sys.path.insert(0, os.path.join(parentdir, \'../../src\'))\n+\n+# sys.path.insert(0, os.path.join(parentdir, \'../../seglink\'))\n import model_vgg16\n \n+\n+# import model_cnn\n parser = argparse.ArgumentParser(description=\'\')\n parser.add_argument(\'--model_scope\', default=\'vgg16\',\n                     help=\'Scope for the tensorflow model.\')\n-parser.add_argument(\'--ckpt_path\', default=\'../model/VGG_ILSVRC_16_layers_ssd.ckpt\',\n+parser.add_argument(\'--ckpt_path\', default=\'../data/VGG_ILSVRC_16_layers_ssd.ckpt\',\n                     help=\'Checkpoint save path.\')\n parser.add_argument(\'--caffe_weights_path\', default=\'../model/VGG_ILSVRC_16_layers_weights.pkl\',\n                     help=\'weights dump path.\')\n@@ -23,6 +27,7 @@ def convert_caffemodel_to_ckpt():\n \n   # create network\n   vgg16 = model_vgg16.Vgg16Model()\n+  # vgg16 = model_cnn.SsdVgg16()\n   model_scope = args.model_scope\n   vgg16.build_model(tf.placeholder(tf.float32, shape=[32,3,300,300]), scope=model_scope)\n \ndiff --git a/tool/convert_caffe_model/dump_caffemodel_weights.py b/tool/convert_caffe_model/dump_caffemodel_weights.py\nindex 75b6cb6..94fe797 100644\n--- a/tool/convert_caffe_model/dump_caffemodel_weights.py\n+++ b/tool/convert_caffe_model/dump_caffemodel_weights.py\n@@ -1,7 +1,8 @@\n import numpy as np\n import joblib\n import argparse\n-\n+import os\n+import sys\n parser = argparse.ArgumentParser(description=\'\')\n parser.add_argument(\'--caffe_root\', help=\'Caffe root directory.\')\n parser.add_argument(\'--prototxt_path\', help=\'Model prototxt path.\')\ndiff --git a/tool/convert_caffe_model/my_convert_caffemodel_to_ckpt.py b/tool/convert_caffe_model/my_convert_caffemodel_to_ckpt.py\nindex 21a2f0d..ffb0170 100644\n--- a/tool/convert_caffe_model/my_convert_caffemodel_to_ckpt.py\n+++ b/tool/convert_caffe_model/my_convert_caffemodel_to_ckpt.py\n@@ -11,7 +11,7 @@ sys.path.insert(0, os.path.join(parentdir, \'../../seglink\'))\n # import model_vgg16\n \n \n-import model_cnn\n+import model_cnn_ckpt\n parser = argparse.ArgumentParser(description=\'\')\n parser.add_argument(\'--model_scope\', default=\'vgg16\',\n                     help=\'Scope for the tensorflow model.\')\n@@ -27,7 +27,7 @@ def convert_caffemodel_to_ckpt():\n \n   # create network\n   # vgg16 = model_vgg16.Vgg16Model()\n-  vgg16 = model_cnn.SsdVgg16()\n+  vgg16 = model_cnn_ckpt.SsdVgg16()\n   model_scope = args.model_scope\n   vgg16.build_model(tf.placeholder(tf.float32, shape=[32,3,300,300]), scope=model_scope)\n \ndiff --git a/tool/create_datasets.py b/tool/create_datasets.py\nindex df7e25e..59e501a 100644\n--- a/tool/create_datasets.py\n+++ b/tool/create_datasets.py\n@@ -81,35 +81,41 @@ def create_synthtext_dataset(save_path, data_root, shuffle=False, n_max=None):\n \n   # load gt.mat\n   print(\'Loading gt.mat ...\')\n+  # \xe4\xbb\x8emat\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xad\xe8\xaf\xbb\xe5\x8f\x96\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe6\xad\xa4\xe5\xa4\x84\xe8\xaf\xbb\xe5\x8f\x96\xe7\x9a\x84\xe6\x98\xafgt\xe6\x95\xb0\xe6\x8d\xae\xe3\x80\x82Load MATLAB file\n   gt = sio.loadmat(os.path.join(data_root, \'gt.mat\'))\n+  # \xe6\xa0\xb7\xe6\x9c\xac\xe6\x95\xb0\n   n_samples = gt[\'wordBB\'].shape[1]\n-\n+  # \xe5\xb0\x86mat\xe6\xa0\xbc\xe5\xbc\x8f\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xbatfrecord\xe6\xa0\xbc\xe5\xbc\x8f\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\n   writer = tf.python_io.TFRecordWriter(save_path)\n   print(\'Start writing to %s\' % save_path)\n-\n+  #\xe8\xae\xbe\xe5\xae\x9a\xe6\x88\x91\xe4\xbb\xac\xe9\x9c\x80\xe8\xa6\x81\xe9\x87\x87\xe6\xa0\xb7\xe7\x9a\x84\xe6\xa0\xb7\xe6\x9c\xac\xe6\x95\xb0\n   if n_max is not None:\n     n_samples = min(n_max, n_samples)\n-\n+  # \xe5\xaf\xb9\xe6\xa0\xb7\xe6\x9c\xac\xe6\x98\xaf\xe5\x90\xa6\xe8\xbf\x9b\xe8\xa1\x8c\xe9\x9a\x8f\xe6\x9c\xba\xe6\x89\x93\xe4\xb9\xb1\xef\xbc\x8c\xe6\xa0\xb7\xe6\x9c\xac\xe6\x8e\x92\xe5\xba\x8f\xe6\x88\x96\xe8\x80\x85\xe6\x89\x93\xe4\xb9\xb1\xe5\x90\x8e\xe7\x9a\x84\xe9\xa1\xba\xe5\xba\x8f\xe4\xb8\xbaindices\n   if shuffle:\n     indices = np.random.permutation(n_samples)\n   else:\n     indices = np.arange(n_samples)\n-\n+  # Tqdm \xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe5\xbf\xab\xe9\x80\x9f,\xe5\x8f\xaf\xe6\x89\xa9\xe5\xb1\x95\xe7\x9a\x84Python\xe8\xbf\x9b\xe5\xba\xa6\xe6\x9d\xa1,\xe5\x8f\xaf\xe4\xbb\xa5\xe5\x9c\xa8 Python \xe9\x95\xbf\xe5\xbe\xaa\xe7\x8e\xaf\xe4\xb8\xad\xe6\xb7\xbb\xe5\x8a\xa0\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbf\x9b\xe5\xba\xa6\xe6\x8f\x90\xe7\xa4\xba\xe4\xbf\xa1\xe6\x81\xaf,\n   for i in tqdm(range(n_samples)):\n     idx = indices[i]\n+    # \xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe7\x9b\xb8\xe5\xaf\xb9\xe8\xb7\xaf\xe5\xbe\x84\n     image_rel_path = str(gt[\'imnames\'][0, idx][0])\n+    # \xe5\xb0\x86\xe5\x9b\xbe\xe7\x89\x87\xe8\xb7\xaf\xe5\xbe\x84\xe8\xbd\xac\xe6\x8d\xa2\xe6\x88\x90\xe4\xb8\xba\xe7\xbb\x9d\xe5\xaf\xb9\xe8\xb7\xaf\xe5\xbe\x84\n     image_path = os.path.join(data_root, image_rel_path)\n     # load image jpeg data\n     with open(image_path, \'rb\') as f:\n       image_jpeg = f.read()\n-    # word polygons\n+    # word polygons\xe3\x80\x82mat[\'wordBB\'][0] \xe6\x94\xbebbox\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae\xe4\xbf\xa1\xe6\x81\xaf\xef\xbc\x8c\xe5\xbc\xa0\xe9\x87\x8f\xe7\x9a\x84\xe7\xbb\xb4\xe5\xba\xa6\xe6\x98\xaf2\xc3\x974\xc3\x97NWORD_i\xe5\x9b\xbe\xe7\x89\x87\xe4\xb8\xad\xe5\x8c\x85\xe5\x90\xab\xe7\x9a\x84word\xe6\x95\xb0\xe9\x87\x8f\xef\xbc\x8c\xe5\xae\x9e\xe9\x99\x85\xe6\x93\x8d\xe4\xbd\x9c\xe4\xb8\xad\xe4\xb8\x80\xe5\xae\x9a\xe8\xa6\x81\xe5\xb0\x8f\xe5\xbf\x83\xe8\xb6\x85\xe5\x87\xba\xe5\xae\x83\xe7\x9a\x84\xe5\x80\xbc\xe5\x9b\xbe\xe7\x89\x87\xe5\xa4\xa7\xe5\xb0\x8f\xe8\x8c\x83\xe5\x9b\xb4\n     word_polygons = gt[\'wordBB\'][0, idx]\n     if word_polygons.ndim == 2:\n       word_polygons = np.expand_dims(word_polygons, axis=2)\n+    # \xe6\x8d\xa2\xe6\x8e\x89\xe4\xbd\x8d\xe7\xbd\xae\xef\xbc\x8c\xe7\xbb\xb4\xe5\xba\xa6\xe5\x8f\x98\xe6\x88\x90NWORD_i\xc3\x974\xc3\x972\n     word_polygons = np.transpose(word_polygons, axes=[2,1,0])\n+    # \xe8\xae\xa1\xe7\xae\x97\xe6\x96\x87\xe6\x9c\xac\xe8\xaf\x8d\xe7\x9a\x84\xe4\xb8\xaa\xe6\x95\xb0\n     n_words = word_polygons.shape[0]\n     word_polygons_flat = [float(o) for o in word_polygons.flatten()]\n-    # words\n+    # words\xef\xbc\x8c\xe6\x96\x87\xe6\x9c\xac\n     text = gt[\'txt\'][0, idx]\n     words = []\n     for text_line in text:\n@@ -255,6 +261,7 @@ class DatasetCreator(object):\n     return example\n \n   def create(self):\n+    # \xe5\x88\x9b\xe5\xbb\xbatf record\xe6\xa0\xbc\xe5\xbc\x8f\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\n     self._read_list()\n     print(\'Start creating dataset with {} examples. Output path: {}\'.format(\n           self.n_samples, self.save_path))\n@@ -269,7 +276,7 @@ class DatasetCreator(object):\n         print(\'Progress %d / %d\' % (i, self.n_samples))\n     print(\'Done creating %d samples\' % count)\n \n-\n+# \xe7\xbb\xa7\xe6\x89\xbf\xe8\x87\xaaDatasetCreator\n class DatasetCreator_Icdar2015Incidental(DatasetCreator):\n   def __init__(self, save_path, data_root, training=True, shuffle=True):\n     self.save_path = save_path\n@@ -320,7 +327,7 @@ class DatasetCreator_Icdar2015Incidental(DatasetCreator):\n       \'word_polygons\': word_polygons}\n     return annot_dict\n \n-\n+# \xe7\xbb\xa7\xe6\x89\xbf\xe8\x87\xaaDatasetCreator\n class DatasetCreator_Icdar2013(DatasetCreator):\n   def __init__(self, save_path, data_root, training, shuffle=False):\n     self.save_path = save_path\n@@ -526,7 +533,8 @@ if __name__ == \'__main__\':\n   #                          shuffle=True)\n \n   # ICDAR 2015 incidental\n-  ic15_data_root = \'/mnt/datasets/scene_text/icdar_2015_incidental/\'\n+  # \xe5\xb0\x86ICDAR2015 incidental\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe8\xbd\xac\xe6\x8d\xa2\xe6\x88\x90\xe4\xb8\xbatf\xe6\xa0\xbc\xe5\xbc\x8f\n+  ic15_data_root = \'/apps/yinhong/paper/data/\'\n   creator_ic15_train = DatasetCreator_Icdar2015Incidental(\n       \'../data/icdar_2015_incidental_train.tf\',\n       ic15_data_root,\n'
2019-01-03 15:37:59,256 [INFO ]  Anchor sizes: [11.84210526, 23.68421053, 45.0, 90.0, 150.0, 285.0]
2019-01-03 15:37:59,275 [INFO ]  Added training dataset #0: ../data/icdar_2015_incidental_train.tf
2019-01-03 15:37:59,388 [INFO ]  Batch size 32; capacity: 1600; min_after_dequeue: 96
2019-01-03 15:37:59,388 [INFO ]  Batch size is 16 on each of the 2 GPUs
2019-01-03 15:37:59,405 [INFO ]  Using SGD optimizer. Momentum=0.9
2019-01-03 15:37:59,405 [INFO ]  Setting up tower 0
2019-01-03 15:38:00,349 [INFO ]  Setting up tower 1
2019-01-03 15:38:01,502 [WARNI]  From solver.py:189: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Please use tf.global_variables instead.
2019-01-03 15:38:02,292 [INFO ]  Initializing model
2019-01-03 15:38:02,757 [INFO ]  Restoring parameters from ../premodel/ILVSR_VGG_16_FC_REDUCED/VGG_ILSVRC_16_layers_ssd.ckpt
2019-01-03 15:38:03,636 [INFO ]  VGG16 parameters loaded from ../premodel/ILVSR_VGG_16_FC_REDUCED/VGG_ILSVRC_16_layers_ssd.ckpt
2019-01-03 15:38:03,638 [INFO ]  Training loop started
